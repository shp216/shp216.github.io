---
layout: single
title: "Pytorch 쓰면서 오류났던 점들 정리"
categories: PyTorch
tag: [Pytorch]
toc: true
author_profile: false
Typora-root-url: ../
use_math: true


---

1. Model 설정할 때 return 값 확실하게 해두기!
2. Crossentropyloss 사용 시 target의 shape은 [n,1]이면 안되고 [n, ]여야 한다!! -> input_size: [batch_size, class_size], output_size: [class_size]
3. MSELoss 사용시 target의 shape은 [n,1]이어야 값이 훨씬 잘나옴!! -> [n, ]이면 unsqueeze를 써서 [n, 1]로 바꿔야 한다!!!

4.이건 느낌이지만 data의개수가 10000개도 되지 않는다면 dropout을 쓰면 효율이 안좋은것 같다..

5. 데이터 형태가 csv파일로 되있지 않는 경우 data의 이름을 통해 직접 dataset에 넣어줘야한다! -> 데이터 경로를 os, os.path를 통해 지정해줘야함 -> 추가적인 공부 필요!
6. cnn에서 직접 모델 구성하는 것 외에도 pretrained된 모델이 많음(googlenet, alexnet, vgg등) -> 형태보고 초기값 설정과 마지막 output layer값을 조정해줘야함
7. Gpu에 큰 메모리가 할당되면 학습 불가 -> dataloader만들 때 batch크기, num_workers 개수로 조정(num_workers개수가 많을 수록 gpu에 할당되는 량이 적다고 논문에 써있음) + adam이 gpu 많이 잡아먹음..
8. 실무에서는 dataset 라이브러리를 이용하여 pytorch에서 데이터를 구성한다고 함 -> 이 부분 계속 연습하면서 학습 중
9. csv파일 읽을 때 header = None 처리 하면 column값이 순서대로 0부터 지정됌! + column값들이 행으로 아래로 내려온다!

+) 모델을 기깔나게 만드는 것보다 데이터 전처리를 공부하는 것이 현재는 더 효율이 좋다고 판단된다 -> transforms 이용해서 data 전처리 하는 법 공부!



Cnn, rnn 공부를 다 해보고 자연어나 영상 중에 한 분야에 대해 더 깊이 공부해봐야 할것 같다

Ex) 자연어 -> bert, gpt-3등 최신 기술 들 논문 보면서 구현해보고 해야 함! -> 할게 많음…

-> 여기까지가 1년전에 공부했던 부분인데 지금을 되돌아보면 ... 생각보다 발전한 부분이 많이 없다고 생각한다. 내년 이맘쯤에는 제발 많이 발전했다고 느꼈으면 좋을거같다. 정신 좀 차려야할듯... 진짜 1년 간 한 게 없는거 같은 느낌이다 ㅠ



10. Mnist load시 Mnist Dataset.data로 접근했더니 transform에 ToTensor를 적용했음에도 정규화가 안되서 찾아봤더니 해답은 이렇다고 한다.

    **이미지 타입**: `train_dataset.data[0]`과 같이 데이터셋에서 이미지를 직접 접근하면, 이미지가 파이토치 텐서가 아니라 파이썬의 `PIL.Image.Image` 형식일 수 있습니다. 이 경우에는 이미지가 정규화되기 전의 원본 형태로 출력됩니다. 정규화된 텐서를 확인하려면 데이터 로더를 이용하여 이미지를 가져와야 합니다.

11. reshape과 view함수에 대한 차이

    기능은 동일하나 view함수는 사용불가능한 경우가 발생하기에 왠만하면 reshape쓰는 것을 추천한다고 한다.

12. squeeze함수와 flatten함수의 차이

    squeeze함수는 1로 되어있는 부분을 다 없애준다. ex) (1,3,4) -> (3,4) , (1,4,5,1) -> (4,5)

    flatten 함수는 뭐든 다 1차원으로 만들어버린다 ex) (1,3,4) -> (12,), (1,4,5,1) -> (20,)
---
layout: single
title: "[Paper Review] DDPM(Denoising Diffusion Probabilistic Models)"
categories: Diffusion
tag: [Diffusion]
toc: true
author_profile: false
Typora-root-url: ../
use_math: true
---

* Paper: Denoising Diffusion Probabilistic Models ; [arxiv](https://arxiv.org/abs/2006.11239)	

## DDPM 

Diffusion Process는 *Markov Chain과 Gaussian distribution*으로 기반으로 image로 부터 noise를 더해가는 과정인 Diffusion process와 이 process를 보고 학습을 통해 noise로 부터 이미지를 생성해내는 denoising process로 이루어져 있다. DDPM 모델은 학습 파라미터들을 **부분적으로 상수로 정의함**으로써 그 수를 줄여 모델 학습 과정을 단순화하여 Diffusion Model 발전의 시초가 된 모델이다. DDPM 논문의 핵심은 neural network로 표현되는 p 모델이 q를 보고 noise를 걷어내는 과정을 학습하는 것이다.

![Diffusion](/images/2023-07-03-DDPM/Diffusion.png)

### Diffusion Process (Forward Process)

![Forward Process](/images/2023-07-03-DDPM/Forward Process.png)

$ q(x_{1:T}\|x_{0}) $ : q는 이미지로부터 noise를 주입하는 과정으로 Markov Chain에 따라 0 step부터 T step까지 $\beta_{t}$를 이용하여 확률분포로 나타내었다. $\beta_{t}$는 Beta scheduling(Cosine Scheduling이 가장 성능이 좋다고 한다)을 이용하여 미리 constant하게 정해놓았기에 Diffusion Process에서는 trainable parameter가 없다. Diffusion Process는 Denoising Process를 진행하는 Network의 본보기로 사용되는 것이다. 

$ q(x_{t}\|x_{t-1}) $ : <u>$x_{t-1}$의 mean $ \mu_{t-1} $, std $ \sigma_{t-1} $</u> 을 알고 있을 때 $x_{t}$에 대한 확률분포이다.		$x_{t-1}$은 Gaussian 분포를 띠기에, $x_{t-1}$를 나타내기 위해서는 $ \mu_{t-1} $, $ \sigma_{t-1} $를 알면된다. <br/><u>$x_{t-1}$의 mean $ \mu_{t-1} $, std $ \sigma_{t-1} $</u> 을 알고 있을 때 $x_{t}$의 mean과 std는 다음과 같다.

<u>$ \mu_{t} $ = $\sqrt{1-\beta_{t}} x_{t-1}$,&nbsp; $ \sigma_{t} $ = $ \beta_{t}$ </u>

$x_{t}$는 $ \mu_{t} $와 $ \sigma_{t} $로 표현되는 Gaussian 분포에서 샘플링한 결과이지만 Backpropagation을 위해 VAE(Various Autoencoder)에서 사용된 기법인 Reparameterization Trick<br/>($x_{t}$ = $ \mu_{t-1} $ + $ \sigma_{t-1}$$\epsilon$ )을 이용하면 다음과 같이 나타낼 수 있다.

<u>$x_{t}$  =  $\sqrt{1-\beta_{t}} x_{t-1}$ + $ \beta_{t}$$\epsilon$</u>


=> **Diffusion process를 보고 어떻게 denoising network p가 denoising process가 되게 학습시킬 수 있을까?** 이것이 DDPM의 핵심이다!






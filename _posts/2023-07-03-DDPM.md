---
layout: single
title: "[Paper Review] DDPM(Denoising Diffusion Probabilistic Models)"
categories: Diffusion
tag: [Diffusion]
toc: true
author_profile: false
Typora-root-url: ../
use_math: true
---

## DDPM 

* Paper: Denoising Diffusion Probabilistic Models ; [arxiv](https://arxiv.org/abs/2006.11239)	

Diffusion Process는 *Markov Chain과 Gaussian distribution*을 기반으로 image로 부터 noise를 더해가는 과정인 Diffusion process와 이 process를 보고 학습을 통해 noise로 부터 이미지를 생성해내는 denoising process로 이루어져 있다. DDPM 모델은 학습 파라미터들을 **부분적으로 상수로 정의함**으로써 그 수를 줄여 모델 학습 과정을 단순화하여 Diffusion Model 발전의 시초가 된 모델이다. DDPM 논문의 핵심은 neural network로 표현되는 p 모델이 q를 보고 noise를 걷어내는 과정을 학습하는 것이다.

![Diffusion](/images/2023-07-03-DDPM/Diffusion.png)

### Diffusion Process (Forward Process)

![Forward Process](/images/2023-07-03-DDPM/Forward Process.png)

$ q(x_{1:T}\|x_{0}) $ : q는 이미지로부터 noise를 주입하는 과정으로 Markov Chain에 따라 0 step부터 T step까지 $\beta_{t}$를 이용하여 확률분포로 나타내었다. $\beta_{t}$는 Beta scheduling(Cosine Scheduling이 가장 성능이 좋다고 한다)을 이용하여 미리 constant하게 정해놓았기에 Diffusion Process에서는 trainable parameter가 없다. Diffusion Process는 Denoising Process를 진행하는 Network의 본보기로 사용되는 것이다. 

$ q(x_{t}\|x_{t-1}) $ : <u>$x_{t-1}$의 mean $ \mu_{t-1} $, std $ \sigma_{t-1} $</u> 을 알고 있을 때 $x_{t}$에 대한 확률분포이다.		$x_{t-1}$은 Gaussian 분포를 띠기에, $x_{t-1}$를 나타내기 위해서는 $ \mu_{t-1} $, $ \sigma_{t-1} $를 알면된다. <br/><u>$x_{t-1}$의 mean $ \mu_{t-1} $, std $ \sigma_{t-1} $</u> 을 알고 있을 때 $x_{t}$의 mean과 std는 다음과 같다.

<u>$ \mu_{t} $ = $\sqrt{1-\beta_{t}} x_{t-1}$,&nbsp; $ \sigma_{t} $ = $ \beta_{t}$ </u>

$x_{t}$는 $ \mu_{t} $와 $ \sigma_{t} $로 표현되는 Gaussian 분포에서 샘플링한 결과이지만 Backpropagation을 위해 VAE(Various Autoencoder)에서 사용된 기법인 Reparameterization Trick<br/>($x_{t}$ = $ \mu_{t-1} $ + $ \sigma_{t-1}$$\epsilon$ )을 이용하면 다음과 같이 나타낼 수 있다.

<u>$x_{t}$  =  $\sqrt{1-\beta_{t}} x_{t-1}$ + $ \beta_{t}$$\epsilon$</u>


=> **<span style="background-color:#fff5b1"><span style="color: black">Diffusion process를 보고 어떻게 Denoising network p가 denoising process가 되게 학습시킬 수 있을까?</span></span>**  이것이 DDPM의 핵심이다!

### Denoising Process (Reverse Process)

![Denoising process](/images/2023-07-03-DDPM/Denoising process.png)

Denoising Process의 과정은 $p_{\theta}(x_{t-1}\|x_{t})$로 나타낼 수 있다. Denoising network p는 Forward Process q를 통해 학습하는데 상식적으로는 $q(x_{t-1}\|x_{t})$를 학습한다고 판단할 수 있다. 하지만 $q(x_{t}\|x_{t-1})$을 통해 $q(x_{t-1}\|x_{t})$을 알수는 없다. **<span style="background-color:#fff5b1"><span style="color: black">그렇다면 Denoising Network는 어떤 방식으로 학습을 진행해야 할까?</span></span>** 

Denoising Network p가 나타내는 확률분포로 부터 원본 이미지 $x_{0}$가 나올 Likelihood가 최대가 되도록 만들면된다. 즉, $p_{\theta}(x_{0})$의 log-likelihood가 최대 혹은 Negative log-likelihhod가 최소가 되도록 학습이 이루어지면 된다. 식으로 표현하면 다음과 같다. <br/> 

***$\underset{\theta}{argmax}$&nbsp;$log(p_{\theta}(x_{0}))$ = $\underset{\theta}{argmin}$&nbsp;$-log(p_{\theta}(x_{0}))$***

하지만 Denoising Network p는 $q(x_{t-1}\|x_{t})$를 모사하기 위해 설정한 것이므로 $log(p_{\theta}(x_{0}))$는 intractable하다! 그래서 바로 구하지 못하고 Evidence of Lower Bound (ELBO)를 이용하여 Network의 학습방향을 설정한다. 이에 대한 풀이과정은 다음과 같다. ~~말도 안되게 기네요..~~


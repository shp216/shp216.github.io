---
layout: single
title: "LSTM 공부한거 정리"
categories: PyTorch
tag: [Pytorch]
toc: true
author_profile: false
Typora-root-url: ../
use_math: true



---

# LSTM(Long Short Term Memory)

RNN(Recurrent Neural Network)은 이전정보를 기억하며 네트워크를 계속해서 학습시키는 모델로 시계열 데이터에 큰 강점을 지닌 모델이다. 이전 step에서의 정보 $h_{t-1}$과 현재 step에서의 입력 값 $x_t$를 입력으로 받아 모델에 넣어주면 $h_t$가 도출되고 이를 다음 step에 이용하며 계속해서 다음 step으로 전달하여 연산하는 방식이다. 그래서 time-series data 처리에 큰 강점을 가지고 있다. 

하지만, RNN에서 Back-Propagation을 통해 weight를 update할 경우, time step t가 일정이상으로 커지면 최종 time step T부터 0까지 미분을 진행하며 BackPropagation이 진행되기에 기울기 소실(Gradient Vanishing) 문제가 발생한다. 즉, 비교적 빠른 time step에 대한 정보가 고려되지 않는 문제가 발생한다. 이를 해결하기 위해 RNN보다는 좀 더 복잡한 방식으로 구성되어 있는 LSTM이 제안되었다. LSTM은 은닉층의 메모리 셀에 Input Gate, Forget Gate, Output Gate를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정한다. 요약하면 LSTM은 hidden state를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 cell state라는 값을 추가하였다. 

<img src="/images/2023-08-09-LSTM/LSTM.png" alt="LSTM" style="zoom: 50%;" />

  

## Cell State 

![Cell state](/images/2023-08-09-LSTM/Cell state.png)

Cell state는 LSTM에서 추가된 또다른 hidden state라고 봐도 무방하다. Cell state는 RNN에서 발생한 gradient vanishing을 해결하는 중요한 역할을 담당한다. Cell state 역시 hidden state와 마찬가지로 이전 step의 결과를 다음 step의 입력값으로 넣어준다. Cell state의 주된 역할을 LSTM에 존재하는 Gate들과 상호작용하며 선택적으로 정보를 활용한다는 점이다. Cell State의 값은 Input Gate, Forget Gate의 값들을 이용하여 구하게 되는데 식은 다음과 같다. $f_t$는 Forget Gate에서 도출된 값이고 $i_t, g_t$는 Input Gate에서 도출된 값이다. 

<u>$c_t = f_t \otimes c_{t-1} + i_t \otimes g_t$</u>

### Input Gate

![Input Gate](/images/2023-08-09-LSTM/Input Gate.png)

